{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c608738",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef9d6254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import re \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm as tqdm\n",
    "n_model=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa4f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", r\" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", r\" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", r\" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "# reads the content of the file passed as an argument.\n",
    "# if limit > 0, this function will return only the first \"limit\" sentences in the file.\n",
    "def loadTexts(filename, limit=-1):\n",
    "    dataset=[]\n",
    "    with open(filename, encoding=\"utf-8\") as f:\n",
    "        line = f.readline()\n",
    "        cpt=1\n",
    "        skip=0\n",
    "        while line :\n",
    "            cleanline = clean_str(f.readline()).split()\n",
    "\n",
    "            \n",
    "            if cleanline: \n",
    "                dataset.append(cleanline)\n",
    "            else: \n",
    "                line = f.readline()\n",
    "                skip+=1\n",
    "                continue\n",
    "            if limit > 0 and cpt >= limit: \n",
    "                break\n",
    "            line = f.readline()\n",
    "            cpt+=1        \n",
    "\n",
    "        print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
    "        \n",
    "    for i in range(len(dataset)):\n",
    "        # remove the last two tokens BUT ONLY if they are actually label + ID\n",
    "        if dataset[i][-2].isdigit() and dataset[i][-1].isalnum():\n",
    "            dataset[i] = dataset[i][:-2]\n",
    "\n",
    "        dataset[i] = [w for w in dataset[i] if w != \",\"] #remove single commas\n",
    "    \n",
    "\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53dae84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  21706  lines from  train.tsv  /  0  lines discarded\n",
      "Load  2714  lines from  test.tsv  /  1  lines discarded\n",
      "Load  2714  lines from  dev.tsv  /  0  lines discarded\n"
     ]
    }
   ],
   "source": [
    "train_data_text = loadTexts(\"train.tsv\")\n",
    "test_data_text = loadTexts(\"test.tsv\")\n",
    "dev_data_text = loadTexts(\"dev.tsv\")\n",
    "\n",
    "for i in range(len(dev_data_text)):\n",
    "    dev_data_text[i] = dev_data_text[i][:-2] # remove the \",\" and other index at the end of each sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cf9f827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['you', 'know', 'the', 'answer', 'man', 'you', 'are', 'programmed', 'to', 'capture', 'those', 'codes', 'they', 'send', 'you', 'don', 't', 'avoid', 'them'], ['the', 'economy', 'is', 'heavily', 'controlled', 'and', 'subsidized', 'by', 'the', 'government', 'in', 'any', 'case', 'i', 'was', 'poking', 'at', 'the', 'lack', 'of', 'nuance', 'in', 'us', 'politics'], ['thank', 'you', 'for', 'your', 'vote', 'of', 'confidence', 'but', 'we', 'statistically', 'ca', \"n't\", 'get', 'to'], ['there', 'it'], ['good', 'we', 'do', \"n't\", 'want', 'more', 'thrash', 'liberal', 'offspring', 'in'], ['i', 'went', 'to', 'a', 'destination', 'wedding', 'being', 'the', 'only', 'single', 'person', 'promised', 'to', 'never', 'put', 'myself', 'in', 'that', 'situation'], ['like', 'this', 'just', 'cuz', 'of', 'the', 'name', 'rhymes', 'background', 'raps', 'but', 'dude', 'your', 'name', 'is', 'sick'], ['as', 'an', 'anesthesia', 'resident', 'this', 'made', 'me', 'blow', 'air', 'out', 'my', 'nose', 'at', 'an', 'accelerated', 'rate', 'for', 'several', 'seconds', 'take', 'your', 'damn', 'upvote', 'you'], ['did', 'you', 'hear', 'the', 'reason', 'for', 'this', '\\\\?', 'because', 'they', 'are', 'concerned', 'about'], ['i', 'do', \"n't\", 'necessarily', 'hate', 'them', 'but', 'then', 'again', 'i', 'dislike', 'it', 'when', 'people', 'breed', 'while', 'knowing', 'how', 'harsh', 'life'], ['downvoted', 'to', 'hell', 'but', 'i', 'understand', 'your', 'experience'], ['say', 'that', 'you', 'like'], ['oh', 'shoot', 'im', 'sorry', 'to', 'hear', 'that', 'was', 'it', 'someone', 'close', 'to', 'you'], [\"i'm\", 'a', 'dudes', 'he', \"'s\", 'a', 'dudes', 'she', \"'s\", 'a', 'dudes', 'we', \"'re\", 'all'], ['kawhi', 'and'], ['i', 'have', 'bought', 'this', 'but', 'not', 'played', \"i'm\", 'scared'], ['i', 'checked', 'the', 'dudes', 'profile', 'before', 'i', 'posted', 'this', 'it', 's', 'not', 'he', 's'], ['name', 'making', 'name', 'even'], ['ehhh', 'it', \"'s\", 'an', 'opinion', 'it', \"'s\", 'not', 'wrong', 'or', 'right', 'just', 'highly', 'unpopular', 'and'], ['lots', 'of', 'them', 'to', 'mature', 'thank', 'you', 'for', 'half']]\n"
     ]
    }
   ],
   "source": [
    "print(dev_data_text[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57228a84",
   "metadata": {},
   "source": [
    "### Word dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90941665",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordDict:\n",
    "    # constructor, words must be a set containing all words\n",
    "    def __init__(self, words):\n",
    "        assert type(words) == set\n",
    "        # TODO\n",
    "        self.words=sorted(list(words))\n",
    "        self.word_dict_={w: i for i, w in enumerate(self.words)}\n",
    "        self.id_dict_={i: w for i, w in enumerate(self.words)}\n",
    "        \n",
    "        # return the integer associated with a word\n",
    "    def word_to_id(self, word):\n",
    "        assert type(word) == str\n",
    "        return self.word_dict_[word]\n",
    "        # TODO\n",
    "\n",
    "        # return the word associated with an integer\n",
    "    def id_to_word(self, idx):\n",
    "        assert type(idx) == int\n",
    "        # TODO\n",
    "        return self.id_dict_[idx]\n",
    "    \n",
    "        # number of word in the dictionnary\n",
    "    def __len__(self):\n",
    "        # TODO\n",
    "        return len(self.word_dict_)\n",
    "\n",
    "\n",
    "        \n",
    "train_words = set()\n",
    "train_words.update([\"<bos>\", \"<eos>\",\"<unk>\"])\n",
    "\n",
    "for sentence in train_data_text:\n",
    "    train_words.update(sentence)\n",
    "\n",
    "word_dict = WordDict(train_words)\n",
    "\n",
    "vocab_size=len(word_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6dc296",
   "metadata": {},
   "source": [
    "### Tensoring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70a7acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorize_sentence(sentence,n=n_model):\n",
    "    \"\"\"\n",
    "    Tensorizing function\n",
    "    Args: sentence: list of words as text\n",
    "    Returns: output_tensor: Tensors of type long, encoding the words via index of the word dictionnary\n",
    "    \"\"\"\n",
    "    start_index=word_dict.word_dict_[\"<bos>\"]\n",
    "    end_index=word_dict.word_dict_[\"<eos>\"]\n",
    "    output_tensor=[word_dict.word_dict_.get(word, word_dict.word_dict_[\"<unk>\"]) for word in sentence]\n",
    "    output_tensor=[start_index]*(n-1)+output_tensor+[end_index] # pad accoring to n to ensure correct first n-gram\n",
    "    return output_tensor\n",
    "\n",
    "train_data = [tensorize_sentence(sentence) for sentence in train_data_text]\n",
    "test_data = [tensorize_sentence(sentence) for sentence in test_data_text]\n",
    "dev_data = [tensorize_sentence(sentence) for sentence in dev_data_text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c762c",
   "metadata": {},
   "source": [
    "### Generate training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77acaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(training_data,n=n_model):\n",
    "    \"\"\"\n",
    "    Generate context,target pairs to train the MLP on\n",
    "    Args: \n",
    "        traning_data: list of words as indexes, \n",
    "        n size of the Ngram  model\n",
    "    \n",
    "    Returns:         \n",
    "        X: list of context vectors of shape (n-1,)\n",
    "        y: list of target word IDs\n",
    "    \"\"\"\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for sentence in training_data:\n",
    "        for index,word in enumerate(sentence) :\n",
    "            if index>= n-1:\n",
    "                training_words=sentence[index-(n-1):index]\n",
    "                target_word=word\n",
    "                X.append(training_words)\n",
    "                y.append(target_word)\n",
    "\n",
    "    return X,y\n",
    "\n",
    "X_train,y_train=generate_pairs(train_data)\n",
    "X_train=th.tensor(X_train, dtype=th.long)\n",
    "y_train=th.tensor(y_train, dtype=th.long)\n",
    "\n",
    "X_test,y_test=generate_pairs(test_data)\n",
    "X_test=th.tensor(X_test, dtype=th.long)\n",
    "y_test=th.tensor(y_test, dtype=th.long)\n",
    "\n",
    "print(f\"Training dataset shape: {X_train.shape}\")\n",
    "print(f\"Test dataset shape: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9032c563",
   "metadata": {},
   "source": [
    "# Perpexity Class\n",
    "\n",
    "\n",
    "Perplexity correponds to how surprised the model is by the test text, it is inversly correlated with the likelihood of the text given the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "389c2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perplexity:\n",
    "\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.total_log_prob=0.0\n",
    "        self.count = 0\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.total_log_prob=0.0\n",
    "        self.count = 0\n",
    "\n",
    "\n",
    "    def add_sentence(self, log_probs):\n",
    "        \"\"\"\n",
    "        log_probs: 1D tensor or list of log probabilities of \n",
    "                   all words in a sentence.\n",
    "                   We just add the to the total log probability\n",
    "        \"\"\"\n",
    "        self.total_log_prob += log_probs.sum().item()\n",
    "        self.count += len(log_probs)\n",
    "\n",
    "\n",
    "    def compute_perplexity(self):\n",
    "        return math.exp(- self.total_log_prob / self.count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d90c30f",
   "metadata": {},
   "source": [
    "# Neural N Gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85abded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNgram(nn.Module):\n",
    "    def __init__(self,n=n_model,embedding_dim=100,hidden_dim=16):\n",
    "        super().__init__()\n",
    "        self.n=n\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim=embedding_dim\n",
    "        \n",
    "        # Small MLP classifier \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear((self.n-1)*embedding_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, vocab_size)\n",
    "        )\n",
    "\n",
    "        #Logging objects\n",
    "        self.loss_history=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        \"\"\"\n",
    "        Forward pass for Neural Ngram.\n",
    "\n",
    "        Args:\n",
    "            inputs: LongTensor of shape (batch_size, n-1)\n",
    "                    where each entry is a token index.\n",
    "\n",
    "        Returns:\n",
    "            logits: Tensor of shape (batch_size,vocab_size)\n",
    "        \"\"\"\n",
    "        # Convert tokens to embeddings Shape (batch_size,n,embedding size) â†’ (batch, seq_len, embedding_dim)\n",
    "        embeds = self.emb(inputs)\n",
    "        embeds = embeds.view(inputs.size(0), self.embedding_dim * (self.n-1))         # flatten (batch_size, 2*emb_dim) to correct mlp size\n",
    "        logits=self.mlp(embeds)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    def fit(self,X_train,y_train,epochs,lr=0.001,batch_size=128):\n",
    "        self.train()\n",
    "        optimizer = th.optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        for epoch in tqdm(range(epochs)):\n",
    "            \n",
    "            #Shuffle data to maximise randomness\n",
    "            perm = th.randperm(len(X_train))\n",
    "            X_train = X_train[perm]\n",
    "            y_train = y_train[perm]\n",
    "\n",
    "            total_loss=0.0\n",
    "\n",
    "            for start in range(0, len(X_train), batch_size):\n",
    "                optimizer.zero_grad()\n",
    "                xb = X_train[start:start + batch_size]\n",
    "                yb = y_train[start:start + batch_size]\n",
    "\n",
    "                logits=self.forward(xb)\n",
    "                loss=criterion(logits,yb)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss+= loss.item()*xb.size(0)\n",
    "\n",
    "            avg_loss = total_loss/len(X_train)\n",
    "            self.loss_history.append(avg_loss)\n",
    "            print(f\"Loss at epoch {epoch}: {avg_loss}\")\n",
    "\n",
    "    def get_perplexity(self,X_test,y_test):\n",
    "        \"\"\"\n",
    "        Get perplexity for the model on the test set by getting the log-probs of correct words \n",
    "        \"\"\"\n",
    "        ppx=Perplexity()\n",
    "        logits=self.forward(X_test)\n",
    "        log_probs=th.log_softmax(logits, dim=1)\n",
    "        correct_lp=log_probs[range(len(y_test)), y_test]  \n",
    "        ppx.add_sentence(correct_lp)\n",
    "        \n",
    "        return ppx.compute_perplexity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795e33ef",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bce0cb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at epoch 0: 6.393155891688182\n",
      "Loss at epoch 1: 5.835974637164718\n",
      "Loss at epoch 2: 5.70080176796483\n",
      "Loss at epoch 3: 5.617829701404216\n",
      "Loss at epoch 4: 5.555850700407181\n"
     ]
    }
   ],
   "source": [
    "nngram=NeuralNgram()\n",
    "\n",
    "nngram.fit(X_train=X_train,y_train=y_train,epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b11ad018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1cc121b2480>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.plot(nngram.loss_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623fcc71",
   "metadata": {},
   "source": [
    "implement unknown word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7264220a",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
