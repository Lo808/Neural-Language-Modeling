{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c608738",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef9d6254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import re \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fa4f2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\?'\n",
      "<>:16: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:17: SyntaxWarning: invalid escape sequence '\\)'\n",
      "<>:18: SyntaxWarning: invalid escape sequence '\\?'\n",
      "C:\\Users\\maloc\\AppData\\Local\\Temp\\ipykernel_5832\\3242241957.py:16: SyntaxWarning: invalid escape sequence '\\('\n",
      "  string = re.sub(r\"\\(\", \" \\( \", string)\n",
      "C:\\Users\\maloc\\AppData\\Local\\Temp\\ipykernel_5832\\3242241957.py:17: SyntaxWarning: invalid escape sequence '\\)'\n",
      "  string = re.sub(r\"\\)\", \" \\) \", string)\n",
      "C:\\Users\\maloc\\AppData\\Local\\Temp\\ipykernel_5832\\3242241957.py:18: SyntaxWarning: invalid escape sequence '\\?'\n",
      "  string = re.sub(r\"\\?\", \" \\? \", string)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "# reads the content of the file passed as an argument.\n",
    "# if limit > 0, this function will return only the first \"limit\" sentences in the file.\n",
    "def loadTexts(filename, limit=-1):\n",
    "    dataset=[]\n",
    "    with open(filename) as f:\n",
    "        line = f.readline()\n",
    "        cpt=1\n",
    "        skip=0\n",
    "        while line :\n",
    "            cleanline = clean_str(f.readline()).split()\n",
    "\n",
    "            \n",
    "            if cleanline: \n",
    "                dataset.append(cleanline)\n",
    "            else: \n",
    "                line = f.readline()\n",
    "                skip+=1\n",
    "                continue\n",
    "            if limit > 0 and cpt >= limit: \n",
    "                break\n",
    "            line = f.readline()\n",
    "            cpt+=1        \n",
    "\n",
    "        print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
    "        \n",
    "    for i in range(len(dataset)):\n",
    "        dataset[i] = dataset[i][:-2] # remove the label at the end of each sentence\n",
    "        if dataset[i]==[',']:\n",
    "            dataset[i]=[]\n",
    "    \n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def make_loader(data, labels, batch_size=32):\n",
    "    X = th.stack(data)      # (N, L)\n",
    "    y = labels              # (N,)\n",
    "    dataset = TensorDataset(X, y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "53dae84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  21706  lines from  train.tsv  /  0  lines discarded\n",
      "Load  2714  lines from  test.tsv  /  1  lines discarded\n",
      "Load  2714  lines from  dev.tsv  /  0  lines discarded\n"
     ]
    }
   ],
   "source": [
    "train_data = loadTexts(\"train.tsv\")\n",
    "test_data = loadTexts(\"test.tsv\")\n",
    "dev_data = loadTexts(\"dev.tsv\")\n",
    "\n",
    "for i in range(len(dev_data)):\n",
    "    dev_data[i] = dev_data[i][:-2] # remove the \",\" and other index at the end of each sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2cf9f827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['you', 'know', 'the', 'answer', 'man', ',', 'you', 'are', 'programmed', 'to', 'capture', 'those', 'codes', 'they', 'send', 'you', ',', 'don', 't', 'avoid', 'them', '!'], ['the', 'economy', 'is', 'heavily', 'controlled', 'and', 'subsidized', 'by', 'the', 'government', 'in', 'any', 'case', ',', 'i', 'was', 'poking', 'at', 'the', 'lack', 'of', 'nuance', 'in', 'us', 'politics', 'today'], ['thank', 'you', 'for', 'your', 'vote', 'of', 'confidence', ',', 'but', 'we', 'statistically', 'ca', \"n't\", 'get', 'to'], ['there', 'it'], ['good', 'we', 'do', \"n't\", 'want', 'more', 'thrash', 'liberal', 'offspring', 'in'], ['i', 'went', 'to', 'a', 'destination', 'wedding', 'being', 'the', 'only', 'single', 'person', 'promised', 'to', 'never', 'put', 'myself', 'in', 'that', 'situation', 'again'], ['like', 'this', 'just', 'cuz', 'of', 'the', 'name', 'rhymes', 'background', 'raps', 'but', 'dude', 'your', 'name', 'is', 'sick'], ['as', 'an', 'anesthesia', 'resident', 'this', 'made', 'me', 'blow', 'air', 'out', 'my', 'nose', 'at', 'an', 'accelerated', 'rate', 'for', 'several', 'seconds', 'take', 'your', 'damn', 'upvote', 'you', 'bastard'], ['did', 'you', 'hear', 'the', 'reason', 'for', 'this', '\\\\?', 'because', 'they', 'are', 'concerned', 'about', 'inventory'], ['i', 'do', \"n't\", 'necessarily', 'hate', 'them', ',', 'but', 'then', 'again', ',', 'i', 'dislike', 'it', 'when', 'people', 'breed', 'while', 'knowing', 'how', 'harsh', 'life', 'is'], ['downvoted', 'to', 'hell', 'but', 'i', 'understand', 'your', 'experience', 'salute'], ['say', 'that', 'you', 'like', 'her'], ['oh', 'shoot', ',', 'im', 'sorry', 'to', 'hear', 'that', 'was', 'it', 'someone', 'close', 'to', 'you', '\\\\?'], [\"i'm\", 'a', 'dudes', ',', 'he', \"'s\", 'a', 'dudes', ',', 'she', \"'s\", 'a', 'dudes', ',', 'we', \"'re\", 'all'], ['kawhi', 'and'], ['i', 'have', 'bought', 'this', 'but', 'not', 'played', \"i'm\", 'scared'], ['i', 'checked', 'the', 'dudes', 'profile', 'before', 'i', 'posted', 'this', ',', 'it', 's', 'not', 'he', 's'], ['name', 'making', 'name', 'even'], ['ehhh', ',', 'it', \"'s\", 'an', 'opinion', ',', 'it', \"'s\", 'not', 'wrong', 'or', 'right', ',', 'just', 'highly', 'unpopular', 'and'], ['lots', 'of', 'them', 'to', 'mature', 'thank', 'you', 'for', 'half']]\n"
     ]
    }
   ],
   "source": [
    "print(dev_data[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57228a84",
   "metadata": {},
   "source": [
    "### Word dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "90941665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18657"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WordDict:\n",
    "    # constructor, words must be a set containing all words\n",
    "    def __init__(self, words):\n",
    "        assert type(words) == set\n",
    "        # TODO\n",
    "        self.words=sorted(list(words))\n",
    "        self.word_dict_={w: i for i, w in enumerate(self.words)}\n",
    "        self.id_dict_={i: w for i, w in enumerate(self.words)}\n",
    "        \n",
    "        # return the integer associated with a word\n",
    "    def word_to_id(self, word):\n",
    "        assert type(word) == str\n",
    "        return self.word_dict_['word']\n",
    "        # TODO\n",
    "\n",
    "        # return the word associated with an integer\n",
    "    def id_to_word(self, idx):\n",
    "        assert type(idx) == int\n",
    "        # TODO\n",
    "        return self.id_dict_['idx']\n",
    "    \n",
    "        # number of word in the dictionnary\n",
    "    def __len__(self):\n",
    "        # TODO\n",
    "        return len(self.word_dict_)\n",
    "\n",
    "\n",
    "        \n",
    "train_words = set()\n",
    "for sentence in train_data:\n",
    "    train_words.update(sentence)\n",
    "    train_words.update([\"<bos>\", \"<eos>\"])\n",
    "\n",
    "word_dict = WordDict(train_words)\n",
    "\n",
    "len(word_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6dc296",
   "metadata": {},
   "source": [
    "### Tensoring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7acd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d90c30f",
   "metadata": {},
   "source": [
    "# N Gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85abded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ngram:\n",
    "    def __init__(self,n):\n",
    "        self.n=n\n",
    "        self.word_count={}\n",
    "        pass\n",
    "\n",
    "    def create_ngrams(self,sentence):\n",
    "        words = sentence.split()\n",
    "        ngrams = {}\n",
    "        for i in range(len(words) - self.n + 1):\n",
    "            ngram = tuple(words[i:i + self.n])\n",
    "            if ngram not in ngrams:\n",
    "                ngrams[ngram] = 0\n",
    "            ngrams[ngram] += 1\n",
    "\n",
    "        return ngrams\n",
    "    \n",
    "    def \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7264220a",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
