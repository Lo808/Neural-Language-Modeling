{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c608738",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9d6254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import re \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from \n",
    "import pandas as pd\n",
    "n_model=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1fa4f2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize a sentence\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", r\" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", r\" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", r\" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "# reads the content of the file passed as an argument.\n",
    "# if limit > 0, this function will return only the first \"limit\" sentences in the file.\n",
    "def loadTexts(filename, limit=-1):\n",
    "    dataset=[]\n",
    "    with open(filename) as f:\n",
    "        line = f.readline()\n",
    "        cpt=1\n",
    "        skip=0\n",
    "        while line :\n",
    "            cleanline = clean_str(f.readline()).split()\n",
    "\n",
    "            \n",
    "            if cleanline: \n",
    "                dataset.append(cleanline)\n",
    "            else: \n",
    "                line = f.readline()\n",
    "                skip+=1\n",
    "                continue\n",
    "            if limit > 0 and cpt >= limit: \n",
    "                break\n",
    "            line = f.readline()\n",
    "            cpt+=1        \n",
    "\n",
    "        print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
    "        \n",
    "    for i in range(len(dataset)):\n",
    "        # remove the last two tokens BUT ONLY if they are actually label + ID\n",
    "        if dataset[i][-2].isdigit() and dataset[i][-1].isalnum():\n",
    "            dataset[i] = dataset[i][:-2]\n",
    "\n",
    "        dataset[i] = [w for w in dataset[i] if w != \",\"] #remove single commas\n",
    "    \n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "def make_loader(data, labels, batch_size=32):\n",
    "    X = th.stack(data)      # (N, L)\n",
    "    y = labels              # (N,)\n",
    "    dataset = TensorDataset(X, y)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53dae84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  21706  lines from  train.tsv  /  0  lines discarded\n",
      "Load  2714  lines from  test.tsv  /  1  lines discarded\n",
      "Load  2714  lines from  dev.tsv  /  0  lines discarded\n"
     ]
    }
   ],
   "source": [
    "train_data_text = loadTexts(\"train.tsv\")\n",
    "test_data_text = loadTexts(\"test.tsv\")\n",
    "dev_data_text = loadTexts(\"dev.tsv\")\n",
    "\n",
    "for i in range(len(dev_data_text)):\n",
    "    dev_data_text[i] = dev_data_text[i][:-2] # remove the \",\" and other index at the end of each sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf9f827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['you', 'know', 'the', 'answer', 'man', 'you', 'are', 'programmed', 'to', 'capture', 'those', 'codes', 'they', 'send', 'you', 'don', 't', 'avoid', 'them'], ['the', 'economy', 'is', 'heavily', 'controlled', 'and', 'subsidized', 'by', 'the', 'government', 'in', 'any', 'case', 'i', 'was', 'poking', 'at', 'the', 'lack', 'of', 'nuance', 'in', 'us', 'politics'], ['thank', 'you', 'for', 'your', 'vote', 'of', 'confidence', 'but', 'we', 'statistically', 'ca', \"n't\", 'get', 'to'], ['there', 'it'], ['good', 'we', 'do', \"n't\", 'want', 'more', 'thrash', 'liberal', 'offspring', 'in'], ['i', 'went', 'to', 'a', 'destination', 'wedding', 'being', 'the', 'only', 'single', 'person', 'promised', 'to', 'never', 'put', 'myself', 'in', 'that', 'situation'], ['like', 'this', 'just', 'cuz', 'of', 'the', 'name', 'rhymes', 'background', 'raps', 'but', 'dude', 'your', 'name', 'is', 'sick'], ['as', 'an', 'anesthesia', 'resident', 'this', 'made', 'me', 'blow', 'air', 'out', 'my', 'nose', 'at', 'an', 'accelerated', 'rate', 'for', 'several', 'seconds', 'take', 'your', 'damn', 'upvote', 'you'], ['did', 'you', 'hear', 'the', 'reason', 'for', 'this', '\\\\?', 'because', 'they', 'are', 'concerned', 'about'], ['i', 'do', \"n't\", 'necessarily', 'hate', 'them', 'but', 'then', 'again', 'i', 'dislike', 'it', 'when', 'people', 'breed', 'while', 'knowing', 'how', 'harsh', 'life'], ['downvoted', 'to', 'hell', 'but', 'i', 'understand', 'your', 'experience'], ['say', 'that', 'you', 'like'], ['oh', 'shoot', 'im', 'sorry', 'to', 'hear', 'that', 'was', 'it', 'someone', 'close', 'to', 'you'], [\"i'm\", 'a', 'dudes', 'he', \"'s\", 'a', 'dudes', 'she', \"'s\", 'a', 'dudes', 'we', \"'re\", 'all'], ['kawhi', 'and'], ['i', 'have', 'bought', 'this', 'but', 'not', 'played', \"i'm\", 'scared'], ['i', 'checked', 'the', 'dudes', 'profile', 'before', 'i', 'posted', 'this', 'it', 's', 'not', 'he', 's'], ['name', 'making', 'name', 'even'], ['ehhh', 'it', \"'s\", 'an', 'opinion', 'it', \"'s\", 'not', 'wrong', 'or', 'right', 'just', 'highly', 'unpopular', 'and'], ['lots', 'of', 'them', 'to', 'mature', 'thank', 'you', 'for', 'half']]\n"
     ]
    }
   ],
   "source": [
    "print(dev_data_text[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57228a84",
   "metadata": {},
   "source": [
    "### Word dictionnary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90941665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18657"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class WordDict:\n",
    "    # constructor, words must be a set containing all words\n",
    "    def __init__(self, words):\n",
    "        assert type(words) == set\n",
    "        # TODO\n",
    "        self.words=sorted(list(words))\n",
    "        self.word_dict_={w: i+3 for i, w in enumerate(self.words)}\n",
    "        self.id_dict_={i+3: w for i, w in enumerate(self.words)}\n",
    "        \n",
    "        # return the integer associated with a word\n",
    "    def word_to_id(self, word):\n",
    "        assert type(word) == str\n",
    "        return self.word_dict_[word]\n",
    "        # TODO\n",
    "\n",
    "        # return the word associated with an integer\n",
    "    def id_to_word(self, idx):\n",
    "        assert type(idx) == int\n",
    "        # TODO\n",
    "        return self.id_dict_[idx]\n",
    "    \n",
    "        # number of word in the dictionnary\n",
    "    def __len__(self):\n",
    "        # TODO\n",
    "        return len(self.word_dict_)\n",
    "\n",
    "\n",
    "        \n",
    "train_words = set()\n",
    "train_words.update([\"<bos>\", \"<eos>\",\"<unk>\"])\n",
    "\n",
    "for sentence in train_data_text:\n",
    "    train_words.update(sentence)\n",
    "\n",
    "word_dict = WordDict(train_words)\n",
    "\n",
    "vocab_size=len(word_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6dc296",
   "metadata": {},
   "source": [
    "### Tensoring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a7acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorize_sentence(sentence,n=n_model):\n",
    "    \"\"\"\n",
    "    Tensorizing function\n",
    "    Args: sentence: list of words as text\n",
    "    Returns: output_tensor: Tensors of type long, encoding the words via index of the word dictionnary\n",
    "    \"\"\"\n",
    "    start_index=word_dict.word_dict_[\"<bos>\"]\n",
    "    end_index=word_dict.word_dict_[\"<eos>\"]\n",
    "    output_tensor=[word_dict.word_dict_.get(word, word_dict.word_dict_[\"<unk>\"]) for word in sentence]\n",
    "    output_tensor=[start_index]*(n-1)+output_tensor+[end_index] # pad accoring to n to ensure correct first n-gram\n",
    "    return output_tensor\n",
    "\n",
    "train_data = [tensorize_sentence(sentence) for sentence in train_data_text]\n",
    "test_data = [tensorize_sentence(sentence) for sentence in test_data_text]\n",
    "dev_data = [tensorize_sentence(sentence) for sentence in dev_data_text]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46c762c",
   "metadata": {},
   "source": [
    "### Generate training pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77acaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pairs(training_data,n=n_model):\n",
    "    \"\"\"\n",
    "    Generate context,target pairs to train the MLP on\n",
    "    Args: \n",
    "        traning_data: list of words as indexes, \n",
    "        n size of the Ngram  model\n",
    "    \n",
    "    Returns:         \n",
    "        X: list of context vectors of shape (n-1,)\n",
    "        y: list of target word IDs\n",
    "    \"\"\"\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for sentence in training_data:\n",
    "        for index,word in enumerate(sentence) :\n",
    "            if index>= n-1:\n",
    "                training_words=sentence[index-(n-1):index]\n",
    "                target_word=word\n",
    "                X.append(training_words)\n",
    "                y.append(target_word)\n",
    "\n",
    "    return X,y\n",
    "\n",
    "X_train,y_train=generate_pairs(train_data)\n",
    "X_train=th.tensor(X_train, dtype=th.long)\n",
    "y_train=th.tensor(y_train, dtype=th.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d90c30f",
   "metadata": {},
   "source": [
    "# Neural N Gram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85abded",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNgram(nn.Module):\n",
    "    def __init__(self,n=n_model,embedding_dim=100,hidden_dim=16):\n",
    "        super().__init__()\n",
    "        self.n=n\n",
    "        self.emb = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding_dim=embedding_dim\n",
    "        \n",
    "        # Small MLP classifier \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear((self.n-1)*embedding_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, vocab_size)\n",
    "        )\n",
    "\n",
    "        #Logging objects\n",
    "        self.loss_history=[]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        \"\"\"\n",
    "        Forward pass for Neural Ngram.\n",
    "\n",
    "        Args:\n",
    "            inputs: LongTensor of shape (batch_size, n-1)\n",
    "                    where each entry is a token index.\n",
    "\n",
    "        Returns:\n",
    "            logits: Tensor of shape (batch_size,vocab_size)\n",
    "        \"\"\"\n",
    "        # Convert tokens to embeddings Shape (batch_size,n,embedding size) â†’ (batch, seq_len, embedding_dim)\n",
    "        embeds = self.emb(inputs)\n",
    "        embeds = embeds.view(inputs.size(0), self.embedding_dim * (self.n-1))         # flatten (batch_size, 2*emb_dim) to correct mlp size\n",
    "        logits=self.mlp(embeds)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    def fit(self,X_train,y_train,epochs,lr=0.001,batch_size=32):\n",
    "        self.train()\n",
    "        optimizer = th.optim.Adam(self.parameters(), lr=lr)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        for _ in range(epochs):\n",
    "            \n",
    "            #Shuffle data to maximise randomness\n",
    "            perm = th.randperm(len(X_train))\n",
    "            X_train = X_train[perm]\n",
    "            y_train = y_train[perm]\n",
    "\n",
    "            total_loss=0.0\n",
    "\n",
    "            for start in range(0, len(X_train), batch_size):\n",
    "                optimizer.zero_grad()\n",
    "                xb = X_train[start:start + batch_size]\n",
    "                yb = y_train[start:start + batch_size]\n",
    "\n",
    "                logits=self.forward(xb)\n",
    "                loss=criterion(logits,yb)\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss+= loss.item()*xb.size(0)\n",
    "\n",
    "            avg_loss = total_loss/len(X_train)\n",
    "            self.loss_history.append(avg_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623fcc71",
   "metadata": {},
   "source": [
    "implement unknown word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7264220a",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
